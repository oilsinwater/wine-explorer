Red wine
Sensory preference
3
4
5
6
7
8
0
100
300
500
700
White wine
Sensory preference
3
4
5
6
7
8
9
0
500
1000
1500
2000
vectors
+e
-e
0
0
+e
-e
support
Red wine
0
20
40
60
80
100
0
0.5
1
1.5
2
White wine
0
20
40
60
80
100
0
0.5
1
1.5
2
Red wine
0
5
10
15
20
citric acid
density
chlorides
residual sugar
fixed acidity
free sulfur dioxide
volatile acidity
alcohol
total sulfur dioxide
pH
sulphates
White wine
0
5
10
15
20
fixed acidity
chlorides
pH
density
volatile acidity
free sulfur dioxide
total sulfur dioxide
citric acid
residual sugar
alcohol
sulphates
Modelingwinepreferencesbydatamining
fromphysicochemicalproperties
PauloCortez
a
;
\003
Ant\023onioCerdeira
b
FernandoAlmeida
b
TelmoMatos
b
Jos\023eReis
a
;
b
a
DepartmentofInformationSystems/R&DCentreAlgoritmi,Universityof
Minho,4800-058Guimar~aes,Portugal
b
ViticultureCommissionoftheVinhoVerderegion\050CVRVV\051,4050-501Porto,
Portugal
Abstract
Weproposeadataminingapproachtopredicthumanwinetastepreferencesthat
isbasedoneasilyavailableanalyticaltestsatthecerti\014cationstep.Alargedataset
\050whencomparedtootherstudiesinthisdomain\051isconsidered,withwhiteandred
vinhoverde
samples\050fromPortugal\051.Threeregressiontechniqueswereapplied,un-
deracomputationallye\016cientprocedurethatperformssimultaneousvariableand
modelselection.Thesupportvectormachineachievedpromisingresults,outper-
formingthemultipleregressionandneuralnetworkmethods.Suchmodelisuseful
tosupporttheoenologistwinetastingevaluationsandimprovewineproduction.
Furthermore,similartechniquescanhelpintargetmarketingbymodelingconsumer
tastesfromnichemarkets.
Keywords:
Sensorypreferences,Regression,Variableselection,Modelselection,
Supportvectormachines,Neuralnetworks
PreprintsubmittedtoElsevier22May2009
1Introduction
Onceviewedasaluxurygood,nowadayswineisincreasinglyenjoyedbya
widerrangeofconsumers.Portugalisatoptenwineexportingcountrywith
3.17%ofthemarketsharein2005[11].Exportsofits
vinhoverde
wine\050from
thenorthwestregion\051haveincreasedby36%from1997to2007[8].Tosupport
itsgrowth,thewineindustryisinvestinginnewtechnologiesforbothwine
makingandsellingprocesses.Winecerti\014cationandqualityassessmentare
keyelementswithinthiscontext.Certi\014cationpreventstheillegaladulteration
ofwines\050tosafeguardhumanhealth\051andassuresqualityforthewinemarket.
Qualityevaluationisoftenpartofthecerti\014cationprocessandcanbeused
toimprovewinemaking\050byidentifyingthemostin\015uentialfactors\051andto
stratifywinessuchaspremiumbrands\050usefulforsettingprices\051.
Winecerti\014cationisgenerallyassessedbyphysicochemicalandsensorytests
[10].Physicochemicallaboratorytestsroutinelyusedtocharacterizewinein-
cludedeterminationofdensity,alcoholorpHvalues,whilesensorytestsrely
mainlyonhumanexperts.Itshouldbestressedthattasteistheleastun-
derstoodofthehumansenses[25],thuswineclassi\014cationisadi\016culttask.
Moreover,therelationshipsbetweenthephysicochemicalandsensoryanalysis
arecomplexandstillnotfullyunderstood[20].
Advancesininformationtechnologieshavemadeitpossibletocollect,store
andprocessmassive,oftenhighlycomplexdatasets.Allthisdataholdvalu-
ableinformationsuchastrendsandpatterns,whichcanbeusedtoimprove
\003
Correspondingauthor.E-mailpcortez@dsi.uminho.pt;tel.:+351253510313;fax:
+351253510300.
2
decisionmakingandoptimizechancesofsuccess[28].Datamining\050DM\051tech-
niques[33]aimatextractinghigh-levelknowledgefromrawdata.Thereare
severalDMalgorithms,eachonewithitsownadvantages.Whenmodelingcon-
tinuousdata,thelinear/multipleregression\050MR\051istheclassicapproach.The
backpropagationalgorithmwas\014rstintroducedin1974[32]andlaterpopular-
izedin1986[23].Sincethen,neuralnetworks\050NNs\051havebecomeincreasingly
used.Morerecently,supportvectormachines\050SVMs\051havealsobeenproposed
[4][26].Duetotheirhigher\015exibilityandnonlinearlearningcapabilities,both
NNsandSVMsaregaininganattentionwithintheDM\014eld,oftenattaining
highpredictiveperformances[16][17].SVMspresenttheoreticaladvantages
overNNs,suchastheabsenceoflocalminimainthelearningphase.Ine\013ect,
theSVMwasrecentlyconsideredoneofthemostin\015uentialDMalgorithms
[34].WhiletheMRmodeliseasiertointerpret,itisstillpossibletoextract
knowledgefromNNsandSVMs,givenintermsofinputvariableimportance
[18][7].
WhenapplyingtheseDMmethods,variableandmodelselectionarecritical
issues.Variableselection[14]isusefultodiscardirrelevantinputs,leading
tosimplermodelsthatareeasiertointerpretandthatusuallygivebetter
performances.Complexmodelsmayover\014tthedata,losingthecapability
togeneralize,whileamodelthatistoosimplewillpresentlimitedlearning
capabilities.Indeed,bothNNandSVMhavehyperparametersthatneedto
beadjusted[16],suchasthenumberofNNhiddennodesortheSVMkernel
parameter,inordertogetgoodpredictiveaccuracy\050seeSection2.3\051.
Theuseofdecisionsupportsystemsbythewineindustryismainlyfocused
onthewineproductionphase[12].DespitethepotentialofDMtechniquesto
predictwinequalitybasedonphysicochemicaldata,theiruseisratherscarce
3
andmostlyconsiderssmalldatasets.Forexample,in1991the\134Wine"dataset
wasdonatedintotheUCIrepository[1].Thedatacontain178exampleswith
measurementsof13chemicalconstituents\050e.g.alcohol,Mg\051andthegoalis
toclassifythreecultivarsfromItaly.Thisdatasetisveryeasytodiscriminate
andhasbeenmainlyusedasabenchmarkfornewDMclassi\014ers.In1997[27],
aNNfedwith15inputvariables\050e.g.ZnandMglevels\051wasusedtopredict
sixgeographicwineorigins.Thedataincluded170samplesfromGermany
anda100%predictiveratewasreported.In2001[30],NNswereusedto
classifythreesensoryattributes\050e.g.sweetness\051ofCalifornianwine,based
ongrapematuritylevelsandchemicalanalysis\050e.g.titrableacidity\051.Only
36exampleswereusedanda6%errorwasachieved.Severalphysicochemical
parameters\050e.g.alcohol,density\051wereusedin[20]tocharacterize56samples
ofItalianwine.Yet,theauthorsarguedthatmappingtheseparameterswitha
sensorytastepanelisaverydi\016culttaskandinsteadtheyusedaNNfedwith
datatakenfromanelectronictongue.Morerecently,mineralcharacterization
\050e.g.ZnandMg\051wasusedtodiscriminate54samplesintotworedwine
classes[21].AprobabilisticNNwasadopted,attaining95%accuracy.Asa
powerfullearningtool,SVMhasoutperformedNNinseveralapplications,
suchaspredictingmeatpreferences[7].Yet,inthe\014eldofwinequalityonly
oneapplicationhasbeenreported,wherespectralmeasurementsfrom147
bottlesweresuccessfullyusedtopredict3categoriesofricewineage[35].
Inthispaper,wepresentacasestudyformodelingtastepreferencesbasedon
analyticaldatathatareeasilyavailableatthewinecerti\014cationstep.Build-
ingsuchmodelisvaluablenotonlyforcerti\014cationentitiesbutalsowine
producersandevenconsumers.Itcanbeusedtosupporttheoenologistwine
evaluations,potentiallyimprovingthequalityandspeedoftheirdecisions.
4
Moreover,measuringtheimpactofthephysicochemicaltestsinthe\014nalwine
qualityisusefulforimprovingtheproductionprocess.Furthermore,itcan
helpintargetmarketing[24],i.e.byapplyingsimilartechniquestomodelthe
consumerspreferencesofnicheand/orpro\014tablemarkets.
Themaincontributionsofthisworkare:
\017
Wepresentanovelmethodthatperformssimultaneousvariableandmodel
selectionforNNandSVMtechniques.Thevariableselectionisbasedon
sensitivityanalysis[18],whichisacomputationallye\016cientmethodthat
measuresinputrelevanceandguidesthevariableselectionprocess.Also,we
proposeaparsimonysearchmethodtoselectthebestSVMkernelparameter
withalowcomputationale\013ort.
\017
Wetestsuchapproachinareal-worldapplication,thepredictionof
vinho
verde
wine\050fromtheMinhoregionofPortugal\051tastepreferences,showing
itsimpactinthisdomain.Incontrastwithpreviousstudies,alargedataset
isconsidered,withatotalof4898whiteand1599redsamples.Winepref-
erencesaremodeledunderaregressionapproach,whichpreservestheorder
ofthegrades,andweshowhowthede\014nitionofthetoleranceconceptis
usefulforaccessingdi\013erentperformancelevels.Webelievethatthisinte-
gratedapproachisvaluabletosupportapplicationswhererankedsensory
preferencesarerequired,forexampleinwineormeatqualityassurance.
Thepaperisorganizedasfollows:Section2presentsthewinedata,DMmod-
elsandvariableselectionapproach;inSection3,theexperimentaldesignis
describedandtheobtainedresultsareanalyzed;\014nally,conclusionsaredrawn
inSection4.
5
2Materialsandmethods
2.1Winedata
Thisstudywillconsider
vinhoverde
,auniqueproductfromtheMinho\050north-
west\051regionofPortugal.Mediuminalcohol,isitparticularlyappreciateddue
toitsfreshness\050speciallyinthesummer\051.Thiswineaccountsfor15%ofthe
totalPortugueseproduction[8],andaround10%isexported,mostlywhite
wine.Inthiswork,wewillanalyzethetwomostcommonvariants,whiteand
red\050ros\023eisalsoproduced\051,fromthedemarcatedregionof
vinhoverde
.The
datawerecollectedfromMay/2004toFebruary/2007usingonlyprotected
designationoforiginsamplesthatweretestedattheo\016cialcerti\014cationen-
tity\050CVRVV\051.TheCVRVVisaninter-professionalorganizationwiththe
goalofimprovingthequalityandmarketingofvinhoverde.Thedatawere
recordedbyacomputerizedsystem\050iLab\051,whichautomaticallymanagesthe
processofwinesampletestingfromproducerrequeststolaboratoryandsen-
soryanalysis.Eachentrydenotesagiventest\050analyticalorsensory\051andthe
\014naldatabasewasexportedintoasinglesheet\050.csv\051.
Duringthepreprocessingstage,thedatabasewastransformedinorderto
includeadistinctwinesample\050withalltests\051perrow.Toavoiddiscarding
examples,onlythemostcommonphysicochemicaltestswereselected.Since
theredandwhitetastesarequitedi\013erent,theanalysiswillbeperformed
separately,thustwodatasets
1
werebuiltwith1599redand4898whiteexam-
ples.Table1presentsthephysicochemicalstatisticsperdataset.Regarding
thepreferences,eachsamplewasevaluatedbyaminimumofthreesensory
1
Thedatasetsareavailableat:http://www3.dsi.uminho.pt/pcortez/wine/
6
assessors\050usingblindtastes\051,whichgradedthewineinascalethatranges
from0\050verybad\051to10\050excellent\051.The\014nalsensoryscoreisgivenbytheme-
dianoftheseevaluations.Fig.1plotsthehistogramsofthetargetvariables,
denotingatypicalnormalshapedistribution\050i.e.withmorenormalgrades
thatextremeones\051.
[insertTable1andFig.1aroundhere]
2.2Dataminingapproachandevaluation
Wewilladoptaregressionapproach,whichpreservestheorderoftheprefer-
ences.Forinstance,ifthetruegradeis3,thenamodelthatpredicts4isbetter
thanonethatpredicts7.Aregressiondataset
D
ismadeupof
k
2f
1
;:::;N
g
examples,eachmappinganinputvectorwith
I
inputvariables\050
x
k
1
;:::;x
k
I
\051to
agiventarget
y
k
.Theregressionperformanceiscommonlymeasuredbyan
errormetric,suchasthemeanabsolutedeviation\050MAD\051[33]:
MAD
=
P
N
i
=1
j
y
i
\000
b
y
i
j
=N
\0501\051
where
b
y
k
isthepredictedvalueforthe
k
inputpattern.Theregressionerror
characteristic\050REC\051curve[2]isalsousedtocompareregressionmodels,with
theidealmodelpresentinganareaof1.0.Thecurveplotstheabsoluteerror
tolerance
T
\050
x
-axis\051,versusthepercentageofpointscorrectlypredicted\050the
accuracy\051withinthetolerance\050
y
-axis\051.
Theconfusionmatrixisoftenusedforclassi\014cationanalysis,wherea
C
\002
C
matrix\050
C
isthenumberofclasses\051iscreatedbymatchingthepredicted
values\050incolumns\051withthedesiredclasses\050inrows\051.Foranorderedoutput,
7
thepredictedclassisgivenby
p
i
=
y
i
,if
j
y
i
\000
b
y
i
j\024
T
,else
p
i
=
y
0
i
,where
y
0
i
denotestheclosestclassto
b
y
i
,giventhat
y
0
i
6
=
y
i
.Fromthematrix,several
metricscanbeusedtoaccesstheoverallclassi\014cationperformance,suchas
theaccuracyandprecision\050i.e.thepredictedcolumnaccuracies\051[33].
Theholdoutvalidationiscommonlyusedtoestimatethegeneralizationcapa-
bilityofamodel[19].Thismethodrandomlypartitionsthedataintotraining
andtestsubsets.Theformersubsetisusedto\014tthemodel\050typicallywith2
=
3
ofthedata\051,whilethelatter\050withtheremaining1
=
3\051isusedtocomputethe
estimate.Amorerobustestimationprocedureisthek-foldcross-validation
[9],wherethedataisdividedinto
k
partitionsofequalsize.Onesubsetis
testedeachtimeandtheremainingdataareusedfor\014ttingthemodel.The
processisrepeatedsequentiallyuntilallsubsetshavebeentested.Therefore,
underthisscheme,alldataareusedfortrainingandtesting.However,this
methodrequiresaround
k
timesmorecomputation,since
k
modelsare\014tted.
2.3Dataminingmethods
WewilladoptthemostcommonNNtype,themultilayerperceptron,where
neuronsaregroupedintolayersandconnectedbyfeedforwardlinks[3].For
regressiontasks,thisNNarchitectureisoftenbasedononehiddenlayerof
H
hiddennodeswithalogisticactivationandoneoutputnodewithalinear
function[16]:
b
y
=
w
o;
0
+
o
\000
1
X
j
=
I
+1
1
1+exp\050
\000
P
I
i
=1
x
i
w
j;i
\000
w
j;
0
\051
\001
w
o;i
\0502\051
where
w
i;j
denotestheweightoftheconnectionfromnode
j
to
i
and
o
the
outputnode.Theperformanceissensitivetothetopologychoice\050
H
\051.ANN
8
with
H
=0isequivalenttotheMRmodel.Byincreasing
H
,morecomplex
mappingscanbeperformed,yetanexcessvalueof
H
willover\014tthedata,
leadingtogeneralizationloss.Acomputationallye\016cientmethodtoset
H
is
tosearchthroughtherange
f
0
;
1
;
2
;
3
;:::;H
max
g
\050i.e.fromthesimplestNNto
morecomplexones\051.Foreach
H
value,aNNistrainedanditsgeneralization
estimateismeasured\050e.g.overavalidationsample\051.Theprocessisstopped
whenthegeneralizationdecreasesorwhen
H
reachesthemaximumvalue
\050
H
max
\051.
InSVMregression[26],theinput
x
2<
I
istransformedintoahigh
m
-
dimensionalfeaturespace,byusinganonlinearmapping\050
\036
\051thatdoesnot
needtobeexplicitlyknownbutthatdependsofakernelfunction\050
K
\051.The
aimofaSVMisto\014ndthebestlinearseparatinghyperplane,toleratinga
smallerror\050
\017
\051when\014ttingthedata,inthefeaturespace:
b
y
=
w
0
+
m
X
i
=1
w
i
\036
i
\050
x
\051\0503\051
The
\017
-insensitivelossfunctionsetsaninsensitivetubearoundtheresiduals
andthetinyerrorswithinthetubearediscarded\050Fig.2\051.
[insertFig.2aroundhere]
Wewilladoptthepopulargaussiankernel,whichpresentslessparametersthan
otherkernels\050e.g.polynomial\051[31]:
K
\050
x;x
0
\051=
exp
\050
\000
\015
jj
x
\000
x
0
jj
2
\051
;\015>
0.Under
thissetup,theSVMperformanceisa\013ectedbythreeparameters:
\015
,
\017
and
C
\050a
trade-o\013between\014ttingtheerrorsandthe\015atnessofthemapping\051.Toreduce
thesearchspace,the\014rsttwovalueswillbesetusingtheheuristics[5]:
C
=3
\050forastandardizedoutput\051and
\017
=
b
\033=
p
N
,where
b
\033
=1
:
5
=N
\002
P
N
i
=1
\050
y
i
\000
b
y
i
\051
2
and
b
y
isthevaluepredictedbya3-nearestneighboralgorithm.Thekernel
9
parameter\050
\015
\051producesthehighestimpactintheSVMperformance,with
valuesthataretoolargeortoosmallleadingtopoorpredictions.Apractical
methodtoset
\015
istostartthesearchfromoneoftheextremesandthensearch
towardsthemiddleoftherangewhilethepredictiveestimateincreases[31].
2.4VariableandModelSelection
Sensitivityanalysis[18]isasimpleprocedurethatisappliedafterthetrain-
ingphaseandanalyzesthemodelresponseswhentheinputsarechanged.
OriginallyproposedforNNs,thissensitivitymethodcanalsobeappliedto
otheralgorithms,suchasSVM[7].Let
b
y
a
j
denotetheoutputobtainedby
holdingallinputvariablesattheiraveragevaluesexcept
x
a
,whichvaries
throughitsentirerangewith
j
2f
1
;:::;L
g
levels.Ifagiveninputvariable
\050
x
a
2f
x
1
;:::;x
I
g
\051isrelevantthenitshouldproduceahighvariance\050
V
a
\051.
Thus,itsrelativeimportance\050
R
a
\051canbegivenby:
V
a
=
P
L
j
=1
\050
b
y
a
j
\000
b
y
a
j
\051
2
=
\050
L
\000
1\051
R
a
=
V
a
=
P
I
i
=1
V
i
\002
100\050%\051
\0504\051
Inthiswork,the
R
a
valueswillbeusedtomeasuretheimportanceoftheinputs
andalsotodiscardirrelevantinputs,guidingthevariableselectionalgorithm.
Wewilladoptthepopularbackwardselection,whichstartswithallvariables
anditerativelydeletesoneinputuntilastoppingcriterionismet[14].Yet,
weguidethevariabledeletion\050ateachstep\051bythesensitivityanalysis,ina
variantthatallowsareductionofthecomputationale\013ortbyafactorof
I
\050whencomparedtothestandardbackwardprocedure\051andthatin[18]has
outperformedothermethods\050e.g.backwardandgeneticalgorithms\051.Similarly
10
to[36],thevariableandmodelselectionwillbeperformedsimultaneously,i.e.
ineachbackwarditerationseveralmodelsaresearched,withtheonethat
presentsthebestgeneralizationestimateselected.ForagivenDMmethod,
theoverallprocedureisdepictedbellow:
\0501\051Startwithall
F
=
f
x
1
;:::;x
I
g
inputvariables.
\0502\051Ifthereisahyperparameter
P
2f
P
1
;:::;P
k
g
totune\050e.g.NNorSVM\051,
startwith
P
1
andgothroughtheremainingrangeuntilthegeneralization
estimatedecreases.Computethegeneralizationestimateofthemodelby
usinganinternalvalidationmethod.Forinstance,iftheholdoutmethod
isused,theavailabledataarefurthersplitintotraining\050to\014tthemodel\051
andvalidationsets\050togetthepredictiveestimate\051.
\0503\051After\014ttingthemodel,computetherelativeimportances\050
R
i
\051ofall
x
i
2
F
variablesanddeletefrom
F
theleastrelevantinput.Gotostep4if
thestoppingcriterionismet,otherwisereturntostep2.
\0504\051Selectthebest
F
\050and
P
incaseofNNorSVM\051values,i.e.,theinput
variablesandmodelthatprovidethebestpredictiveestimates.Finally,
retrainthiscon\014gurationwithallavailabledata.
3Empiricalresults
The
R
environment[22]isanopensource,multipleplatform\050e.g.Windows,
Linux\051andhigh-levelmatrixprogramminglanguageforstatisticalanddata
analysis.Allexperimentsreportedinthisworkwerewrittenin
R
andcon-
ductedinaLinuxserver,withanInteldualcoreprocessor.Inparticular,we
adoptedthe
RMiner
[6],alibraryforthe
R
toolthatfacilitatestheuseof
DMtechniquesinclassi\014cationandregressiontasks.
11
Before\014ttingthemodels,thedatawas\014rststandardizedtoazeromeanand
onestandarddeviation[16].
RMiner
usesthee\016cientBFGSalgorithmto
traintheNNs\050
nnetR
package\051,whiletheSVM\014tisbasedontheSequential
MinimalOptimizationimplementationprovidedbyLIBSVM\050
kernlab
pack-
age\051.Weadoptedthedefault
R
suggestions[29].Theonlyexceptionarethe
hyperparameters\050
H
and
\015
\051,whichwillbesetusingtheproceduredescribed
intheprevioussectionandwiththesearchrangesof
H
2f
0
;
1
;:::;
11
g
[36]
and
\015
2f
2
3
;
2
1
;:::;
2
\000
15
g
[31].Whilethemaximumnumberofsearchesis
12/10,inpracticetheparsimonyapproach\050step2ofSection2.4\051willreduce
thisnumbersubstantially.
Regardingthevariableselection,wesettheestimationmetrictothe
MAD
value\050Equation1\051,asadvisedin[31].Toreducethecomputationale\013ort,
weadoptedthesimpler2/3and1/3holdoutsplitastheinternalvalida-
tionmethod.Thesensitivityanalysisparameterwassetto
L
=5,i.e.
x
a
2
f\000
1
:
0
;
\000
0
:
5
;:::;
1
:
0
g
forastandardizedinput.Asareasonablebalancebe-
tweenthepressuretowardssimplermodelsandtheincreaseofcomputational
search,thestoppingcriterionwassetto2iterationswithoutanyimprovement
orwhenonlyoneinputisavailable.
Toevaluatetheselectedmodels,weadopted20runsofthemorerobust5-fold
cross-validation,inatotalof20
\002
5=100experimentsforeachtestedcon\014g-
uration.Statisticalcon\014dencewillbegivenbythet-studenttestatthe95%
con\014dencelevel[13].TheresultsaresummarizedinTable2.Thetestset
errorsareshownintermsofthemeanandcon\014denceintervals.Threemet-
ricsarepresent:
MAD
,theclassi\014cationaccuracyfordi\013erenttolerances\050i.e.
T
=0
:
25,0.5and1.0\051andKappa\050
T
=0
:
5\051.Theselectedmodelsaredescribed
intermsoftheaveragenumberofinputs\050
I
\051andhyperparametervalue\050
H
or
12
\015
\051.Thelastrowshowsthetotalcomputationaltimerequiredinseconds.
[insertTable2andFig.3aroundhere]
Forbothtasksandallerrormetrics,theSVMisthebestchoice.Thedi\013erences
arehigherforsmalltolerancesandinparticularforthewhitewine\050e.g.for
T
=0
:
25,theSVMaccuracyisalmosttwotimesbetterwhencomparedto
othermethods\051.Thise\013ectisclearlyvisiblewhenplottingthefullRECcurves
\050Fig.3\051.TheKappastatistic[33]measurestheaccuracywhencomparedwith
arandomclassi\014er\050whichpresentsaKappavalueof0%\051.Thehigherthe
statistic,themoreaccuratetheresult.Themostpracticaltolerancevaluesare
T
=0
:
5and
T
=1
:
0.Theformertoleranceroundstheregressionresponse
intothenearestclass,whilethelatteracceptsaresponsethatiscorrectwithin
oneofthetwoclosestclasses\050e.g.a3.1valuecanbeinterpretedasgrade3
or4butnot2or5\051.For
T
=0
:
5,theSVMaccuracyimprovementis3.3
ppforredwine\0506.2ppforKappa\051,avaluethatincreasesto12.0ppforthe
whitetask\05020.4ppforKappa\051.TheNNisquitesimilartoMRintheredwine
modeling,thussimilarperformanceswereachieved.Forthewhitedata,amore
complexNNmodel\050
H
=2
:
1\051wasselected,slightlyoutperformingtheMR
results.Regardingthevariableselection,theaveragenumberofdeletedinputs
rangesfrom0.9to1.8,showingthatmostofthephysicochemicaltestsused
arerelevant.Intermsofcomputationale\013ort,theSVMisthemostexpensive
method,particularlyforthelargerwhitedataset.
AdetailedanalysisoftheSVMclassi\014cationresultsispresentedbytheaverage
confusionmatrixesfor
T
=0
:
5\050Table3\051.Tosimplifythevisualization,the3
and9gradepredictionswereomitted,sincethesewerealwaysempty.Mostof
thevaluesareclosetothediagonals\050inbold\051,denotingagood\014tbythemodel.
13
Thetruepredictiveaccuracyforeachclassisgivenbytheprecisionmetric
\050e.g.forthegrade4andwhitewine,precision
T
=0
:
5
=19/\05019+7+4\051=63.3%\051.
Thisstatisticisimportantinpractice,sinceinarealdeploymentsettingthe
actualvaluesareunknownandallpredictionswithinagivencolumnwould
betreatedthesame.Foratoleranceof0.5,theSVMredwineaccuracies
arearound57.7to67.5%intheintermediategrades\0505to7\051andverylow
\0500%/20%\051fortheextremeclasses\0503,8and4\051,whicharelessfrequent\050Fig.
1\051.Ingeneral,thewhitedataresultsarebetter:60.3/63.3%forclasses6and
4,67.8/72.6%forgrades7and5,andasurprising85.5%fortheclass8\050the
exceptionarethe3and9extremeswith0%,notshowninthetable\051.When
thetoleranceisincreased\050
T
=1
:
0\051,highaccuraciesrangingfrom81.9to
100%areattainedforbothwinetypesandclasses4to8.
[insertTable3andFig.4aroundhere]
TheaverageSVMrelativeimportanceplots\050
R
a
values\051oftheanalyticaltests
areshowninFig.4.Itshouldbenotedthatthewhole11inputsareshown,
sinceineachsimulationdi\013erentsetsofvariablescanbeselected.Inseveral
cases,theobtainedresultscon\014rmtheoenologicaltheory.Forinstance,an
increaseinthealcohol\0504thand2ndmostrelevantfactor\051tendstoresultin
ahigherqualitywine.Also,therankingsaredi\013erentwithineachwinetype.
Forinstance,thecitricacidandresidualsugarlevelsaremoreimportantin
whitewine,wheretheequilibriumbetweenthefreshnessandsweettasteis
moreappreciated.Moreover,thevolatileacidityhasanegativeimpact,since
aceticacidisthekeyingredientinvinegar.Themostintriguingresultisthe
highimportanceofsulphates,ranked\014rstforbothcases.Oenologicallythis
resultcouldbeveryinteresting.Anincreaseinsulphatesmightberelatedto
thefermentingnutrition,whichisveryimportanttoimprovethewinearoma.
14
4Conclusionsandimplications
Inrecentyears,theinterestinwinehasincreased,leadingtogrowthofthe
wineindustry.Asaconsequence,companiesareinvestinginnewtechnolo-
giestoimprovewineproductionandselling.Qualitycerti\014cationisacrucial
stepforbothprocessesandiscurrentlylargelydependentonwinetastingby
humanexperts.Thisworkaimsatthepredictionofwinepreferencesfrom
objectiveanalyticalteststhatareavailableatthecerti\014cationstep.Alarge
dataset\050with4898whiteand1599redentries\051wasconsidered,including
vinho
verde
samplesfromthenorthwestregionofPortugal.Thiscasestudywasad-
dressedbytworegressiontasks,whereeachwinetypepreferenceismodeled
inacontinuousscale,from0\050verybad\051to10\050excellent\051.Thisapproachpre-
servestheorderoftheclasses,allowingtheevaluationofdistinctaccuracies,
accordingtothedegreeoferrortolerance\050
T
\051thatisaccepted.
Duetoadvancesinthedatamining\050DM\051\014eld,itispossibletoextractknowl-
edgefromrawdata.Indeed,powerfultechniquessuchasneuralnetworks
\050NNs\051andmorerecentlysupportvectormachines\050SVMs\051areemerging.While
beingmore\015exiblemodels\050i.e.no
apriori
restrictionisimposed\051,theper-
formancedependsonacorrectsettingofhyperparameters\050e.g.numberof
hiddennodesoftheNNarchitectureorSVMkernelparameter\051.Ontheother
hand,themultipleregression\050MR\051iseasiertointerpretthanNN/SVM,with
mostoftheNN/SVMapplicationsconsideringtheirmodelsasblackboxes.
Anotherrelevantaspectisvariableselection,whichleadstosimplermodels
whileoftenimprovingthepredictiveperformance.Inthisstudy,wepresentan
integratedandcomputationallye\016cientapproachtodealwiththeseissues.
SensitivityanalysisisusedtoextractknowledgefromtheNN/SVMmodels,
15
givenintermsofrelativeimportanceoftheinputs.Simultaneousvariableand
modelselectionschemeisalsoproposed,wherethevariableselectionisguided
bysensitivityanalysisandthemodelselectionisbasedonparsimonysearch
thatstartsfromareasonablevalueandisstoppedwhenthegeneralization
estimatedecreases.
Encouragingresultswereachieved,withtheSVMmodelprovidingthebest
performances,outperformingtheNNandMRtechniques,particularlyfor
white
vinhoverde
wine,whichisthemostcommontype.Whenadmitting
onlythecorrectclassi\014edclasses\050
T
=0
:
5\051,theoverallaccuraciesare62.4%
\050red\051and64.6%\050white\051.Itshouldbenotedthatthedatasetscontainsix/seven
classes\050from3to8/9\051.Theseaccuraciesaremuchbetterthantheonesex-
pectedbyarandomclassi\014er.Theperformanceissubstantiallyimprovedwhen
thetoleranceissettoacceptresponsesthatarecorrectwithintheoneofthe
twonearestclasses\050
T
=1
:
0\051,obtainingaglobalaccuracyof89.0%\050red\051and
86.8%\050white\051.Inparticular,forbothtasksthemajorityoftheclassespresent
anindividualaccuracy\050precision\051higherthan90%.
ThesuperiorityofSVMoverNNisprobablyduetothedi\013erencesinthetrain-
ingphase.TheSVMalgorithmguaranteesanoptimum\014t,whileNNtraining
mayfallintoalocalminimum.Also,theSVMcostfunction\050Fig.2\051givesa
linearpenaltytolargeerrors.Incontrast,theNNalgorithmminimizesthesum
ofsquarederrors.Thus,theSVMisexpectedtobelesssensitivetooutliers
andthise\013ectresultsinahigheraccuracyforlowerrortolerances.Asargued
in[15],itisdi\016culttocompareDMmethodsinafairway,withdataanalysts
tendingtofavormodelsthattheyknowbetter.Weadoptedthedefaultsug-
gestionsofthe
R
tool[29],exceptforthehyperparameters\050whichwereset
usingagridsearch\051.Sincethedefaultsettingsaremorecommonlyused,this
16
seemsareasonableassumptionforthecomparison.Nevertheless,di\013erentNN
resultscouldbeachievedifdi\013erenthiddennodeand/orminimizationcost
functionswereused.Underthetestedsetup,theSVMalgorithmprovidedthe
bestresultswhilerequiringmorecomputation.Yet,theSVM\014ttingcanstill
beachievedwithinareasonabletimewithcurrentprocessors.Forexample,
onerunofthe5-foldcross-validationtestingtakesaround26minutesforthe
largerwhitedataset,whichcoversathree-yearcollectionperiod.
Theresultofthisworkisimportantforthewineindustry.Atthecerti\014cation
phaseandbyPortugueselaw,thesensoryanalysishastobeperformedbyhu-
mantasters.Yet,theevaluationsarebasedintheexperienceandknowledgeof
theexperts,whicharepronetosubjectivefactors.Theproposeddata-driven
approachisbasedonobjectivetestsandthusitcanbeintegratedintoa
decisionsupportsystem,aidingthespeedandqualityoftheoenologistper-
formance.Forinstance,theexpertcouldrepeatthetastingonlyifher/his
gradeisfarfromtheonepredictedbytheDMmodel.Ine\013ect,withinthis
domainthe
T
=1
:
0distanceisacceptedasagoodqualitycontrolprocessand,
asshowninthisstudy,highaccuracieswereachievedforthistolerance.The
modelcouldalsobeusedtoimprovethetrainingofoenologystudents.Fur-
thermore,therelativeimportanceoftheinputsbroughtinterestinginsights
regardingtheimpactoftheanalyticaltests.Sincesomevariablescanbecon-
trolledintheproductionprocessthisinformationcanbeusedtoimprovethe
winequality.Forinstance,alcoholconcentrationcanbeincreasedordecreased
bymonitoringthegrapesugarconcentrationpriortotheharvest.Also,the
residualsugarinwinecouldberaisedbysuspendingthesugarfermentation
carriedoutbyyeasts.Moreover,thevolatileacidityproducedduringthemalo-
lacticfermentationinredwinedependsonthelacticbacteriacontrolactivity.
17
Anotherinterestingapplicationistargetmarketing[24].Speci\014cconsumer
preferencesfromnicheand/orpro\014tablemarkets\050e.g.foraparticularcoun-
try\051couldbemeasuredduringpromotioncampaigns\050e.g.freewinetastings
atsupermarkets\051andmodeledusingsimilarDMtechniques,aimingatthe
designofbrandsthatmatchthesemarketneeds.
Acknowledgments
WewouldliketothankCristinaLagidoandtheanonymousreviewersfortheir
helpfulcomments.TheworkofP.CortezissupportedbytheFCTproject
PTDC/EIA/64541/2006.
References
[1]A.AsuncionandD.Newman.UCIMachineLearningRepository,Universityof
California,Irvine,http://www.ics.uci.edu/
\030
mlearn/MLRepository.html,2007.
[2]J.BiandK.Bennett.RegressionErrorCharacteristiccurves.In
Proceedings
of20thInt.Conf.onMachineLearning\050ICML\051
,WashingtonDC,USA,2003.
[3]C.Bishop.
NeuralNetworksforPatternRecognition
.OxfordUniversityPress,
1995.
[4]B.Boser,I.Guyon,andV.Vapnik.Atrainingalgorithmforoptimalmargin
classi\014ers.In
COLT'92:ProceedingsoftheFifthAnnualWorkshopon
ComputationalLearningTheory
,pages144{152,NY,USA,1992.ACM.
[5]V.CherkassyandY.Ma.PracticalSelectionofSVMParametersandNoise
EstimationforSVMRegression.
NeuralNetworks
,17\0501\051:113{126,2004.
18
[6]P.Cortez.RMiner:DataMiningwithNeuralNetworksandSupportVector
MachinesusingR.InR.Rajesh\050Ed.\051,
IntroductiontoAdvancedScienti\014c
SoftwaresandToolboxes
,Inpress.
[7]P.Cortez,M.Portelinha,S.Rodrigues,V.Cadavez,andA.Teixeira.Lamb
MeatQualityAssessmentbySupportVectorMachines.
NeuralProcessing
Letters
,24\0501\051:41{51,2006.
[8]CVRVV.PortugueseWine-VinhoVerde.Comiss~aodeViticulturadaRegi~ao
dosVinhosVerdes\050CVRVV\051,http://www.vinhoverde.pt,July2008.
[9]T.Dietterich.ApproximateStatisticalTestsforComparingSupervised
Classi\014cationLearningAlgorithms.
NeuralComputation
,10\0507\051:1895{1923,
1998.
[10]S.Ebeler.
FlavorChemistry-ThirtyYearsofProgress
,chapterLinking
\015avourchemistrytosensoryanalysisofwine,pages409{422.KluwerAcademic
Publishers,1999.
[11]FAO.FAOSTAT-FoodandAgricultureOrganizationagriculturetradedomain
statistics.http://faostat.fao.org/site/535/DesktopDefault.aspx?PageID=535,
July2008.
[12]J.Ferrer,A.MacCawley,S.Maturana,S.Toloza,andJ.Vera.Anoptimization
approachforschedulingwinegrapeharvestoperations.
InternationalJournal
ofProductionEconomics
,112\0502\051:985{999,2008.
[13]A.Flexer.Statisticalevaluationofneuralnetworksexperiments:Minimum
requirementsandcurrentpractice.In
Proceedingsofthe13thEuropeanMeeting
onCyberneticsandSystemsResearch
,volume2,pages1005{1008,Vienna,
Austria,1996.
[14]I.GuyonandA.Elissee\013.Anintroductiontovariableandfeatureselection.
JournalofMachineLearningResearch
,3\0507{8\051:1157{1182,2003.
19
[15]D.Hand.Classi\014ertechnologyandtheillusionofprogress.
StatisticalScience
,
21\0501\051:1{15,2006.
[16]T.Hastie,R.Tibshirani,andJ.Friedman.
TheElementsofStatisticalLearning:
DataMining,InferenceandPrediction
.Springer-Verlag,NY,USA,2001.
[17]Z.Huang,H.Chen,C.Hsu,W.Chen,andS.Wu.Creditratinganalysiswith
supportvectormachinesandneuralnetworks:amarketcomparativestudy.
DecisionSupportSystems
,37\0504\051:543{558,2004.
[18]R.Kewley,M.Embrechts,andC.Breneman.DataStripMiningfortheVirtual
DesignofPharmaceuticalswithNeuralNetworks.
IEEETransactionsonNeural
Networks
,11\0503\051:668{679,May2000.
[19]M.Kiang.Acomparativeassessmentofclassi\014cationmethods.
Decision
SupportSystems
,35\0504\051:441{454,2003.
[20]A.Legin,A.Rudnitskaya,L.Luvova,Y.Vlasov,C.Natale,andA.D'Amico.
EvaluationofItalianwinebytheelectronictongue:recognition,quantitative
analysisandcorrelationwithhumansensoryperception.
AnalyticaChimica
Acta
,484\0501\051:33{34,2003.
[21]I.Moreno,D.Gonz\023alez-Weller,V.Gutierrez,M.Marino,A.Came\023an,
a.Gonz\023alez,andA.Hardisson.Di\013erentiationoftwoCanaryDOredwines
accordingtotheirmetalcontentfrominductivelycoupledplasmaoptical
emissionspectrometryandgraphitefurnaceatomicabsorptionspectrometry
byusingProbabilisticNeuralNetworks.
Talanta
,72\0501\051:263{268,2007.
[22]RDevelopmentCoreTeam.
R:Alanguageandenvironmentforstatistical
computing
.RFoundationforStatisticalComputing,Vienna,Austria,ISBN
3-900051-00-3,http://www.R-project.org,2008.
[23]D.Rumelhart,G.Hinton,andR.Williams.LearningInternalRepresentations
byErrorPropagation.InD.RulmelhartandJ.McClelland,editors,
Parallel
20
DistributedProcessing:ExplorationsintheMicrostructuresofCognition
,
volume1,pages318{362,MITPress,CambridgeMA,1986.
[24]M.Shaw,C.Subramaniam,G.Tan,andM.Welge.Knowledgemanagement
anddataminingformarketing.
DecisionSupportSystems
,31\0501\051:127{137,2001.
[25]D.SmithandR.Margolskee.Makingsenseoftaste.
Scienti\014cAmerican,
Specialissue
,16\0503\051:84{92,2006.
[26]A.SmolaandB.Scholkopf.Atutorialonsupportvectorregression.
Statistics
andComputing
,14:199{222,2004.
[27]L.Sun,K.Danzer,andG.Thiel.Classi\014cationofwinesamplesbymeansof
arti\014cialneuralnetworksanddiscriminationanalyticalmethods.
Fresenius'
JournalofAnalyticalChemistry
,359\0502\051:143{149,1997.
[28]E.Turban,R.Sharda,J.Aronson,andD.King.
BusinessIntelligence,A
ManagerialApproach
.Prentice-Hall,2007.
[29]W.VenablesandB.Ripley.
ModernAppliedStatisticswithS
.Springer,4th
edition,2003.
[30]S.Vlassides,J.Ferrier,andD.Block.UsingHistoricalDataforBioprocess
Optimization:ModelingWineCharacteristicsUsingArti\014cialNeuralNetworks
andArchivedProcessInformation.
BiotechnologyandBioengineering
,73\0501\051,
2001.
[31]W.Wang,Z.Xu,W.Lu,andX.Zhang.Determinationofthespreadparameter
intheGaussiankernelforclassi\014cationandregression.
Neurocomputing
,
55\0503\051:643{663,2003.
[32]P.Werbos.
Beyondregression:Newtoolsforpredictionandanalysisinthe
behavioralsciences
.PhDthesis,HarvardUniversity,Cambridge,MA,1974.
21
[33]I.H.WittenandE.Frank.
DataMining:PracticalMachineLearningToolsand
TechniqueswithJavaImplementations
.MorganKaufmann,SanFrancisco,CA,
2ndedition,2005.
[34]X.Wu,V.Kumar,J.Quinlan,J.Gosh,Q.Yang,H.Motoda,G.MacLachlan,
A.Ng,B.Liu,P.Yu,Z.Zhou,M.Steinbach,D.Hand,andD.Steinberg.Top
10algorithmsindatamining.
KnowledgeandInformationSystems
,14\0501\051:1{37,
2008.
[35]H.Yu,H.Lin,H.Xu,Y.Ying,B.Li,andX.Pan.PredictionofEnological
ParametersandDiscriminationofRiceWineAgeUsingLeast-SquaresSupport
VectorMachinesandNearInfraredSpectroscopy.
AgriculturalandFood
Chemistry
,56\0502\051:307{313,2008.
[36]M.Yu,M.Shanker,G.Zhang,andM.Hung.Modelingconsumersituational
choiceoflongdistancecommunicationwithneuralnetworks.
DecisionSupport
Systems
,44\0504\051:899{908,2008.
22
Table1
Thephysicochemicaldatastatisticsperwinetype
Attribute\050units\051RedwineWhitewine
MinMaxMeanMinMaxMean
\014xedacidity\050
g
\050
tartaricacid
\051
=dm
3
\0514.615.98.33.814.26.9
volatileacidity\050
g
\050
aceticacid
\051
=dm
3
\0510.11.60.50.11.10.3
citricacid\050
g=dm
3
\0510.01.00.30.01.70.3
residualsugar\050
g=dm
3
\0510.915.52.50.665.86.4
chlorides\050
g
\050
sodiumchloride
\051
=dm
3
\0510.010.610.080.010.350.05
freesulfurdioxide\050
mg=dm
3
\05117214228935
totalsulfurdioxide\050
mg=dm
3
\0516289469440138
density\050
g=cm
3
\0510.9901.0040.9960.9871.0390.994
pH2.74.03.32.73.83.1
sulphates\050
g
\050
potassiumsulphate
\051
=dm
3
\0510.32.00.70.21.10.5
alcohol\050%vol.\0518.414.910.48.014.210.4
23
Table2
Thewinemodelingresults\050testseterrorsandselectedmodels;bestvaluesinbold\051
RedwineWhitewine
MRNNSVMMRNNSVM
MAD0.50
\006
0.000.51
\006
0.00
0.46
\006
0.00
?
0.59
\006
0.000.58
\006
0.00
0.45
\006
0.00
?
Accuracy
T
=0
:
25
\050%\05131.2
\006
0.231.1
\006
0.7
43.2
\006
0.6
?
25.6
\006
0.126.5
\006
0.3
50.3
\006
1.1
?
Accuracy
T
=0
:
50
\050%\05159.1
\006
0.159.1
\006
0.3
62.4
\006
0.4
?
51.7
\006
0.152.6
\006
0.3
64.6
\006
0.4
?
Accuracy
T
=1
:
00
\050%\05188.6
\006
0.188.8
\006
0.2
89.0
\006
0.2
\006
84.3
\006
0.184.7
\006
0.1
86.8
\006
0.2
?
Kappa
T
=0
:
5
\050%\05132.2
\006
0.332.5
\006
0.6
38.7
\006
0.7
?
20.9
\006
0.123.5
\006
0.6
43.9
\006
0.4
?
Inputs\050
I
\0519.29.39.89.69.310.1
Model{
H
=1
\015
=2
0
:
19
{
H
=2
:
1
\015
=2
1
:
55
Time\050
s
\051
518
8475589
551
133930674
?
-Statisticallysigni\014cantunderapairwisecomparisonwithMRandNN.
\006
-Statisticallysigni\014cantunderapairwisecomparisonwithMR.
24
Table3
Theaverageconfusionmatrixes\050
T
=0
:
5\051andprecisionvalues\050
T
=0
:
5and1.0\051
fortheSVMmodel\050boldvaluesdenoteaccuratepredictions\051
Actual
Redwinepredictions
Whitewinepredictions
Class
45678
45678
3
17200
021700
4
1
361510
19
558810
5
3
514
15950
7
833
598190
6
0194
400
440
4235
1812
1443
7
010107
82
1
018414
441
7
8
00108
0
037143
59
9
01320
Precision
T
=0
:
5
20.0%67.5%57.7%58.6%0.0%
63.3%72.6%60.3%67.8%85.5%
Precision
T
=1
:
0
93.8%90.9%86.6%90.2%100%
90.0%93.3%81.9%90.3%96.2%
25
Fig.1.Thehistogramsfortheredandwhitesensorypreferences
26
Fig.2.ExampleofalinearSVMregressionandthe
\017
-insensitivelossfunction
\050adaptedfrom[26]\051
27
Fig.3.Thered\050left\051andwhite\050right\051wineaveragetestsetRECcurves\050SVM{
solidline,NN-graylineandMR{dashedline\051.
28
Fig.4.Thered\050top\051andwhite\050bottom\051wineinputimportancesfortheSVM
model\050in%\051.
29